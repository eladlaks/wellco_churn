{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5331b1",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "from econml.dr import DRLearner\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklift.models import SoloModel\n",
    "\n",
    "from sklift.viz import plot_qini_curve\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from sklift.metrics import qini_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada20e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "repo_root = ([cwd] + list(cwd.parents))[1]\n",
    "\n",
    "# Ensure repo_root is on sys.path so `src.train` can be imported\n",
    "sys.path.append(str(repo_root))\n",
    "from src.process_datasets import create_data, get_web_feats\n",
    "\n",
    "# Load the YAML config file\n",
    "with open(os.path.join(repo_root, \"config.yaml\"), \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a421e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handler import DataHandler\n",
    "from src.trainers import compare_train_test_performance, calculate_qini_auuc\n",
    "from src.data_process import check_column_consistency,check_missing_values,dataset_health_check,check_column_consistency,check_missing_values,analyze_data_drift,dataset_health_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90ebbf",
   "metadata": {},
   "source": [
    "## data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9a57d",
   "metadata": {},
   "source": [
    "### data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a83dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Load data\n",
    "# ----------------------------\n",
    "app_usage = pd.read_csv(repo_root / \"data\" / \"train\" / \"app_usage.csv\")\n",
    "web_visits = pd.read_csv(repo_root / \"data\" / \"train\" / \"web_visits.csv\")\n",
    "claims = pd.read_csv(repo_root / \"data\" / \"train\" / \"claims.csv\")\n",
    "churn_labels = pd.read_csv(repo_root / \"data\" / \"train\" / \"churn_labels.csv\")\n",
    "test_app_usage = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_app_usage.csv\")\n",
    "test_web_visits = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_web_visits.csv\")\n",
    "test_claims = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_claims.csv\")\n",
    "test_churn_labels = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_churn_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a00ab",
   "metadata": {},
   "source": [
    "### data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c43bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_handler = DataHandler(day_first_web=True)\n",
    "test_data_handler = DataHandler(day_first_web=False)\n",
    "X_train, y_train, treatment_train = train_data_handler.get_data(\n",
    "    app_usage, web_visits, claims, churn_labels\n",
    ")\n",
    "X_test, y_test, treatment_test = test_data_handler.get_data(test_app_usage, test_web_visits, test_claims, test_churn_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c422c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "y_train = y_train.fillna(0)\n",
    "y_test = y_test.fillna(0)\n",
    "treatment_train = treatment_train.fillna(0)\n",
    "treatment_test = treatment_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Execution ---\n",
    "dataset_health_check(X_train, X_test)\n",
    "\n",
    "print(\"\\n=========================================\")\n",
    "print(\"          HEALTH CHECK COMPLETE          \")\n",
    "print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a8a9a",
   "metadata": {},
   "source": [
    "### feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Combine X_train with churn and outreach\n",
    "X_train_with_labels = X_train.copy()\n",
    "X_train_with_labels[\"churn\"] = y_train\n",
    "X_train_with_labels[\"outreach\"] = treatment_train\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = X_train_with_labels.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e58bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = X_train_with_labels.corr()\n",
    "\n",
    "# Filter the correlation matrix to show only 'churn' and 'outreach' correlations with all features\n",
    "filtered_corr = corr_matrix.loc[[\"churn\", \"outreach\"]]\n",
    "\n",
    "# Display the filtered correlation matrix\n",
    "plt.figure(figsize=(12, 2))\n",
    "sns.heatmap(filtered_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation of Churn and Outreach with Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation Analysis: Identify Redundant Features\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Get correlation matrix (already computed in previous cell)\n",
    "corr_abs = corr_matrix.abs()\n",
    "\n",
    "# 1. Find highly correlated feature pairs (threshold: 0.8)\n",
    "upper_triangle = np.triu(np.ones(corr_abs.shape), k=1).astype(bool)\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_abs.columns)):\n",
    "    for j in range(i + 1, len(corr_abs.columns)):\n",
    "        if corr_abs.iloc[i, j] > 0.8:\n",
    "            high_corr_pairs.append(\n",
    "                {\n",
    "                    \"Feature_1\": corr_abs.columns[i],\n",
    "                    \"Feature_2\": corr_abs.columns[j],\n",
    "                    \"Correlation\": corr_abs.iloc[i, j],\n",
    "                }\n",
    "            )\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(\"Correlation\", ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HIGHLY CORRELATED FEATURES (|r| > 0.8)\")\n",
    "print(\"=\" * 80)\n",
    "if len(high_corr_df) > 0:\n",
    "    print(high_corr_df.head(20).to_string(index=False))\n",
    "    print(f\"\\nTotal pairs: {len(high_corr_df)}\")\n",
    "else:\n",
    "    print(\"‚úÖ No feature pairs with |correlation| > 0.8\")\n",
    "\n",
    "# 2. Check correlation with target (churn) and treatment (outreach)\n",
    "target_corr = (\n",
    "    corr_matrix[[\"churn\", \"outreach\"]].abs().sort_values(\"churn\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE CORRELATION WITH CHURN & OUTREACH\")\n",
    "print(\"=\" * 80)\n",
    "print(target_corr.head(20).to_string())\n",
    "\n",
    "# 3. Identify features to potentially drop\n",
    "# Rule: If two features correlated > 0.8, drop the one with lower correlation to churn\n",
    "features_to_drop = set()\n",
    "\n",
    "for _, row in high_corr_df.iterrows():\n",
    "    feat1, feat2 = row[\"Feature_1\"], row[\"Feature_2\"]\n",
    "\n",
    "    # Skip if already marked for dropping\n",
    "    if feat1 in features_to_drop or feat2 in features_to_drop:\n",
    "        continue\n",
    "\n",
    "    # Compare correlation with churn\n",
    "    if feat1 in target_corr.index and feat2 in target_corr.index:\n",
    "        corr1 = target_corr.loc[feat1, \"churn\"]\n",
    "        corr2 = target_corr.loc[feat2, \"churn\"]\n",
    "\n",
    "        # Drop the feature with weaker correlation to churn\n",
    "        if corr1 < corr2:\n",
    "            features_to_drop.add(feat1)\n",
    "        else:\n",
    "            features_to_drop.add(feat2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDED FEATURES TO DROP (Redundant + Weak Churn Correlation)\")\n",
    "print(\"=\" * 80)\n",
    "if len(features_to_drop) > 0:\n",
    "    print(f\"Total features to drop: {len(features_to_drop)}\")\n",
    "    print(sorted(features_to_drop))\n",
    "else:\n",
    "    print(\"‚úÖ No redundant features detected\")\n",
    "\n",
    "# 4. Variance Inflation Factor (VIF) check for multicollinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Select only numeric features (exclude target/treatment if still present)\n",
    "numeric_cols = [\n",
    "    col\n",
    "    for col in X_train.columns\n",
    "    if col not in [\"churn\", \"outreach\"] and X_train[col].dtype in [\"float64\", \"int64\"]\n",
    "]\n",
    "\n",
    "# Sample if too many features (VIF is slow)\n",
    "if len(numeric_cols) > 50:\n",
    "    print(\n",
    "        f\"\\n‚ö†Ô∏è  Sampling 50 features for VIF analysis (full dataset has {len(numeric_cols)} features)\"\n",
    "    )\n",
    "    numeric_cols_sample = np.random.choice(numeric_cols, 50, replace=False).tolist()\n",
    "else:\n",
    "    numeric_cols_sample = numeric_cols\n",
    "\n",
    "# Calculate VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = numeric_cols_sample\n",
    "vif_data[\"VIF\"] = [\n",
    "    variance_inflation_factor(X_train[numeric_cols_sample].fillna(0).values, i)\n",
    "    for i in range(len(numeric_cols_sample))\n",
    "]\n",
    "vif_data = vif_data.sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VARIANCE INFLATION FACTOR (VIF) - Top 20 Features\")\n",
    "print(\"VIF > 10 indicates severe multicollinearity\")\n",
    "print(\"=\" * 80)\n",
    "print(vif_data.head(20).to_string(index=False))\n",
    "\n",
    "# 5. PCA Recommendation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PCA vs FEATURE SELECTION RECOMMENDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "n_high_corr = len(high_corr_df)\n",
    "n_vif_high = (vif_data[\"VIF\"] > 10).sum()\n",
    "\n",
    "print(f\"\\nüìä DATASET SUMMARY:\")\n",
    "print(f\"   Total features: {n_features}\")\n",
    "print(f\"   Highly correlated pairs: {n_high_corr}\")\n",
    "print(f\"   Features with VIF > 10: {n_vif_high}\")\n",
    "print(f\"   Recommended to drop: {len(features_to_drop)}\")\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATION:\")\n",
    "\n",
    "if n_high_corr > 20 or n_vif_high > 10:\n",
    "    print(\"   ‚ö†Ô∏è  HIGH MULTICOLLINEARITY DETECTED\")\n",
    "    print(\"   ‚Üí Option 1: Drop redundant features (simple, interpretable)\")\n",
    "    print(\n",
    "        \"   ‚Üí Option 2: Use PCA (loses interpretability, may help with regularization)\"\n",
    "    )\n",
    "    print(\"\\n   For WellCo uplift modeling (per data/wellco_client_brief.txt):\")\n",
    "    print(\"   ‚úÖ RECOMMENDED: Drop redundant features\")\n",
    "    print(\"      - Preserves clinical interpretability (ICD-10 codes, claims_count)\")\n",
    "    print(\"      - LightGBM handles remaining collinearity well\")\n",
    "    print(\"   ‚ùå AVOID PCA for now:\")\n",
    "    print(\"      - Loses domain meaning (can't explain 'PC1' to clinicians)\")\n",
    "    print(\"      - Use only if dimensionality reduction is critical for compute\")\n",
    "else:\n",
    "    print(\"   ‚úÖ MODERATE MULTICOLLINEARITY\")\n",
    "    print(\n",
    "        \"   ‚Üí Drop {0} redundant features (correlation > 0.8)\".format(\n",
    "            len(features_to_drop)\n",
    "        )\n",
    "    )\n",
    "    print(\"   ‚Üí PCA not needed ‚Äî LightGBM handles remaining correlations\")\n",
    "\n",
    "# 6. Save features to drop for reproducibility\n",
    "if len(features_to_drop) > 0:\n",
    "    output_path = repo_root / \"outputs\" / f\"features_to_drop_{timestamp}.txt\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(sorted(features_to_drop)))\n",
    "    print(f\"\\n‚úÖ Redundant features list saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Redundant Features (If Recommended Above)\n",
    "\n",
    "if len(features_to_drop) > 0:\n",
    "    print(f\"Dropping {len(features_to_drop)} redundant features...\")\n",
    "    X_train_reduced = X_train.drop(columns=list(features_to_drop))\n",
    "    X_test_reduced = X_test.drop(columns=list(features_to_drop))\n",
    "\n",
    "    print(f\"Original shape: {X_train.shape}\")\n",
    "    print(f\"Reduced shape:  {X_train_reduced.shape}\")\n",
    "\n",
    "    # Use X_train_reduced for modeling\n",
    "else:\n",
    "    print(\"No features to drop ‚Äî proceed with current feature set\")\n",
    "    X_train_reduced = X_train.copy()\n",
    "    X_test_reduced = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_outreach_grouped = X_train_reduced.groupby(treatment_train).mean()\n",
    "train_data_outreach_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_outreach_grouped = X_train.groupby(treatment_train).mean()\n",
    "train_data_outreach_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87094f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze Heterogeneous Treatment Effects by Segment\n",
    "\n",
    "\n",
    "# Prepare data with labels\n",
    "X_train_with_labels = X_train.copy()\n",
    "X_train_with_labels[\"churn\"] = y_train.values\n",
    "X_train_with_labels[\"outreach\"] = treatment_train.values\n",
    "\n",
    "# Calculate overall ATE for reference\n",
    "overall_ate = (\n",
    "    y_train[treatment_train == 1].mean() - y_train[treatment_train == 0].mean()\n",
    ")\n",
    "\n",
    "# Features to analyze (WellCo clinical priorities)\n",
    "features_ate = X_train.columns.tolist()\n",
    "\n",
    "for feature in features_ate:\n",
    "    if feature not in X_train.columns:\n",
    "        print(f\"\\n‚ö†Ô∏è  Feature '{feature}' not found in X_train, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Check if feature has variation\n",
    "    if X_train[feature].nunique() <= 1:\n",
    "        print(f\"\\n‚ö†Ô∏è  Feature '{feature}' has no variation, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Handle features with few unique values (binary/categorical)\n",
    "    unique_vals = X_train[feature].nunique()\n",
    "\n",
    "    try:\n",
    "        if unique_vals <= 3:\n",
    "            # For binary/low-cardinality features, use actual values as segments\n",
    "            X_train_with_labels[f\"{feature}_level\"] = X_train[feature].astype(str)\n",
    "            segments = sorted(X_train_with_labels[f\"{feature}_level\"].unique())\n",
    "        else:\n",
    "            # For continuous features, create tertiles\n",
    "            X_train_with_labels[f\"{feature}_level\"] = pd.qcut(\n",
    "                X_train[feature],\n",
    "                q=3,\n",
    "                labels=[\"Low\", \"Medium\", \"High\"],\n",
    "                duplicates=\"drop\",  # Handle tied values\n",
    "            )\n",
    "            segments = [\"Low\", \"Medium\", \"High\"]\n",
    "\n",
    "        # Calculate ATE for each segment\n",
    "        segment_ates = []\n",
    "        for segment in segments:\n",
    "            mask = X_train_with_labels[f\"{feature}_level\"] == segment\n",
    "\n",
    "            # Check if we have both treated and control members in this segment\n",
    "            n_treated = (mask & (treatment_train == 1)).sum()\n",
    "            n_control = (mask & (treatment_train == 0)).sum()\n",
    "\n",
    "            if n_treated == 0 or n_control == 0:\n",
    "                print(\n",
    "                    f\"\\n‚ö†Ô∏è  Segment '{segment}' has no treated or control members, skipping...\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            treated_churn = X_train_with_labels.loc[\n",
    "                mask & (treatment_train == 1), \"churn\"\n",
    "            ].mean()\n",
    "            control_churn = X_train_with_labels.loc[\n",
    "                mask & (treatment_train == 0), \"churn\"\n",
    "            ].mean()\n",
    "\n",
    "            segment_ate = treated_churn - control_churn\n",
    "            n_members = mask.sum()\n",
    "\n",
    "            segment_ates.append(\n",
    "                {\n",
    "                    \"Segment\": f\"{segment} {feature}\",\n",
    "                    \"ATE\": segment_ate,\n",
    "                    \"Members\": n_members,\n",
    "                    \"N_Treated\": n_treated,\n",
    "                    \"N_Control\": n_control,\n",
    "                    \"Treated_Churn\": treated_churn,\n",
    "                    \"Control_Churn\": control_churn,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if len(segment_ates) == 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  No valid segments for '{feature}', skipping...\")\n",
    "            continue\n",
    "\n",
    "        ate_df = pd.DataFrame(segment_ates)\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"TREATMENT EFFECT BY {feature.upper()} LEVEL\")\n",
    "        print(\"=\" * 80)\n",
    "        print(ate_df.to_string(index=False))\n",
    "        print(f\"\\nOverall ATE (reference): {overall_ate:.4f}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Visualize\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        x = range(len(ate_df))\n",
    "        colors = [\"#d62728\" if ate > 0 else \"#2ca02c\" for ate in ate_df[\"ATE\"]]\n",
    "\n",
    "        bars = ax.bar(x, ate_df[\"ATE\"], color=colors, alpha=0.7, edgecolor=\"black\")\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for i, (idx, row) in enumerate(ate_df.iterrows()):\n",
    "            ax.text(\n",
    "                i,\n",
    "                row[\"ATE\"],\n",
    "                f'{row[\"ATE\"]:.3f}',\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\" if row[\"ATE\"] > 0 else \"top\",\n",
    "                fontsize=9,\n",
    "            )\n",
    "\n",
    "        # Reference lines\n",
    "        ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=1, alpha=0.3)\n",
    "        ax.axhline(\n",
    "            y=overall_ate,\n",
    "            color=\"blue\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"Overall ATE ({overall_ate:.4f})\",\n",
    "        )\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(ate_df[\"Segment\"], rotation=45, ha=\"right\")\n",
    "        ax.set_ylabel(\"Treatment Effect (Treated Churn - Control Churn)\", fontsize=11)\n",
    "        ax.set_title(\n",
    "            f\"Heterogeneous Treatment Effects by {feature}\\n(Negative = Outreach Reduces Churn)\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.legend(loc=\"best\")\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Clinical interpretation for WellCo\n",
    "        print(f\"\\nüí° CLINICAL INSIGHT for {feature}:\")\n",
    "        best_segment = ate_df.loc[ate_df[\"ATE\"].idxmin()]\n",
    "        worst_segment = ate_df.loc[ate_df[\"ATE\"].idxmax()]\n",
    "\n",
    "        if best_segment[\"ATE\"] < overall_ate:\n",
    "            print(\n",
    "                f\"   ‚úÖ BEST: {best_segment['Segment']} shows strongest benefit (ATE={best_segment['ATE']:.4f})\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      ‚Üí Prioritize outreach for this segment ({best_segment['Members']} members)\"\n",
    "            )\n",
    "\n",
    "        if worst_segment[\"ATE\"] > 0:\n",
    "            print(\n",
    "                f\"   ‚ö†Ô∏è  WORST: {worst_segment['Segment']} shows harm/no benefit (ATE={worst_segment['ATE']:.4f})\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      ‚Üí Avoid outreach for this segment ({worst_segment['Members']} members)\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing '{feature}': {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\n",
    "    \"SUMMARY: Use these insights to target outreach based on uplift model predictions\"\n",
    ")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c7a27",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6ed28",
   "metadata": {},
   "source": [
    "### maunal t learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59113557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------\n",
    "#  dataset\n",
    "# ---------------------------\n",
    "X = X_train\n",
    "treatment = treatment_train\n",
    "y = y_train\n",
    "# ---------------------------\n",
    "# 1. Propensity score estimation\n",
    "# ---------------------------\n",
    "prop_model = LogisticRegression(max_iter=1000)\n",
    "prop_model.fit(X, treatment)\n",
    "propensity_scores = prop_model.predict_proba(X)[:, 1]\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Train separate outcome models (T-learner)\n",
    "# ---------------------------\n",
    "weights_treated = 1 / propensity_scores\n",
    "weights_control = 1 / (1 - propensity_scores)\n",
    "\n",
    "model_treated = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_treated.fit(\n",
    "    X[treatment == 1], y[treatment == 1], sample_weight=weights_treated[treatment == 1]\n",
    ")\n",
    "\n",
    "model_control = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_control.fit(\n",
    "    X[treatment == 0], y[treatment == 0], sample_weight=weights_control[treatment == 0]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Predict outcomes under treatment and control\n",
    "# ---------------------------\n",
    "y_pred_treated = model_treated.predict_proba(X_test)[:, 1]\n",
    "y_pred_control = model_control.predict_proba(X_test)[:, 1]\n",
    "\n",
    "uplift_scores = y_pred_treated - y_pred_control\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Qini curve\n",
    "# ---------------------------\n",
    "def qini_curve(y, treatment, uplift):\n",
    "    \"\"\"Compute Qini curve points\"\"\"\n",
    "    df = pd.DataFrame({\"y\": y, \"t\": treatment, \"uplift\": uplift})\n",
    "    df = df.sort_values(\"uplift\", ascending=False)\n",
    "\n",
    "    cum_treated = df[\"t\"].cumsum()\n",
    "    cum_control = (~df[\"t\"].astype(bool)).cumsum()\n",
    "\n",
    "    cum_outcome_treated = (df[\"y\"] * df[\"t\"]).cumsum()\n",
    "    cum_outcome_control = (df[\"y\"] * (1 - df[\"t\"])).cumsum()\n",
    "\n",
    "    # Qini: difference in cumulative outcomes scaled by group sizes\n",
    "    qini = cum_outcome_treated - cum_outcome_control * (\n",
    "        cum_treated / np.maximum(cum_control, 1)\n",
    "    )\n",
    "    return qini\n",
    "\n",
    "\n",
    "qini = qini_curve(y_test, treatment_test, uplift_scores)\n",
    "\n",
    "\n",
    "# Qini curve subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(1, len(qini) + 1), qini, label=\"Uplift model\")\n",
    "plt.plot([0, len(qini)], [0, qini.max()], \"--\", label=\"Random model\")\n",
    "plt.xlabel(\"Number of individuals targeted\")\n",
    "plt.ylabel(\"Cumulative incremental response\")\n",
    "plt.title(\"Qini Curve\")\n",
    "plt.legend()\n",
    "\n",
    "# compute Qini AUC (area between model qini curve and random model)\n",
    "x = np.arange(1, len(qini) + 1)\n",
    "qini_vals = qini.values\n",
    "\n",
    "model_auc = np.trapz(qini_vals, x)\n",
    "random_line = np.linspace(0, qini_vals.max(), len(qini_vals))\n",
    "random_auc = np.trapz(random_line, x)\n",
    "\n",
    "qini_auc = model_auc - random_auc\n",
    "qini_auc_norm = qini_auc / random_auc if random_auc != 0 else np.nan\n",
    "\n",
    "print(f\"Qini AUC (area over random): {qini_auc:.4f}\")\n",
    "print(f\"Normalized Qini AUC: {qini_auc_norm:.4f}\")\n",
    "\n",
    "# annotate qini plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.annotate(\n",
    "    f\"Qini AUC: {qini_auc:.3f}\\nNorm: {qini_auc_norm:.3f}\",\n",
    "    xy=(0.6 * len(qini_vals), 0.6 * qini_vals.max()),\n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.8),\n",
    ")\n",
    "\n",
    "# second subplot: uplift score distribution for diagnostics\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(uplift_scores, bins=30, color=\"C1\", edgecolor=\"k\")\n",
    "plt.xlabel(\"Uplift score (treated - control)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Uplift score distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795b523",
   "metadata": {},
   "source": [
    "### s learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elastic Net Logistic Regression\n",
    "\n",
    "logistic_elasticnet = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=5000,\n",
    "    class_weight=\"balanced\",\n",
    "    penalty=\"elasticnet\",  # Mix of L1 and L2\n",
    "    solver=\"saga\",\n",
    "    C=0.2,\n",
    "    l1_ratio=0.1,  # 0.5 = equal mix of L1 and L2\n",
    ")\n",
    "\n",
    "slearner_logistic_enet = SoloModel(estimator=logistic_elasticnet)\n",
    "slearner_logistic_enet.fit(X_train, y_train, treatment=treatment_train)\n",
    "\n",
    "uplift_test_enet = slearner_logistic_enet.predict(X_test)\n",
    "test_qini_enet = qini_auc_score(y_test, uplift_test_enet, treatment_test)\n",
    "print(f\"Elastic Net Logistic Test Qini AUC: {test_qini_enet:.4f}\")\n",
    "uplift_train_enet = slearner_logistic_enet.predict(X_train)\n",
    "train_qini_enet = qini_auc_score(y_train, uplift_train_enet, treatment_train)\n",
    "print(f\"Elastic Net Logistic Train Qini AUC: {train_qini_enet:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_train_test_performance(\n",
    "    slearner_logistic_enet,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    treatment_train,\n",
    "    treatment_test,\n",
    "    model_name=\"Baseline S-Learner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea67071",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uplift Score Variance Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UPLIFT SCORE VARIANCE DIAGNOSTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nUplift score range:\")\n",
    "print(f\"  Min: {uplift_test_enet.min():.6f}\")\n",
    "print(f\"  Max: {uplift_test_enet.max():.6f}\")\n",
    "print(f\"  Range: {uplift_test_enet.max() - uplift_test_enet.min():.6f}\")\n",
    "print(f\"  Mean: {uplift_test_enet.mean():.6f}\")\n",
    "print(f\"  Std: {uplift_test_enet.std():.6f}\")\n",
    "\n",
    "print(\n",
    "    f\"\\n‚ö†Ô∏è  ISSUE: Very narrow uplift range ({uplift_test_enet.max() - uplift_test_enet.min():.6f})\"\n",
    ")\n",
    "print(f\"   This indicates weak heterogeneous treatment effects\")\n",
    "print(f\"   Model predicts similar uplift for most members\")\n",
    "\n",
    "# Visualize distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(uplift_test_enet, bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "ax1.axvline(\n",
    "    uplift_test_enet.mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {uplift_test_enet.mean():.6f}\",\n",
    ")\n",
    "ax1.set_xlabel(\"Uplift Score (Predicted Treatment Effect)\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_title(\"Distribution of Uplift Scores (Test Set)\", fontweight=\"bold\")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Box plot with percentiles\n",
    "ax2.boxplot(uplift_test_enet, vert=True)\n",
    "ax2.set_ylabel(\"Uplift Score\")\n",
    "ax2.set_title(\"Uplift Score Distribution (Box Plot)\", fontweight=\"bold\")\n",
    "ax2.grid(alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Add percentile annotations\n",
    "percentiles = [10, 25, 50, 75, 90]\n",
    "for p in percentiles:\n",
    "    val = np.percentile(uplift_test_enet, p)\n",
    "    ax2.axhline(val, color=\"gray\", linestyle=\":\", alpha=0.5)\n",
    "    ax2.text(1.15, val, f\"P{p}: {val:.5f}\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    repo_root / \"outputs\" / f\"uplift_distribution_{timestamp}.png\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Distribution plot saved to: outputs/uplift_distribution_{timestamp}.png\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9a056",
   "metadata": {},
   "source": [
    "### DRLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe31bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Data Loading and Preparation\n",
    "df_test = pd.DataFrame(\n",
    "    {\n",
    "        \"churn\": y_test,\n",
    "        \"outreach\": treatment_test,\n",
    "        \"member_id\": np.arange(10000),  # Placeholder member IDs\n",
    "    }\n",
    ")\n",
    "# -------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. EconML Model Training (DRLearner) - Corrected\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Define the models\n",
    "model_propensity = LogisticRegression(solver=\"lbfgs\")  # P(T|X)\n",
    "# IMPORTANT: Added model_regression for E[Y|X, T], using Regressor for continuous Y\n",
    "model_regression = RandomForestRegressor(\n",
    "    n_estimators=100, max_depth=3, min_samples_leaf=10, random_state=42\n",
    ")\n",
    "model_final = GradientBoostingRegressor(\n",
    "    n_estimators=100, max_depth=3, min_samples_leaf=10, random_state=42\n",
    ")  # Final CATE Estimator (tau(X))\n",
    "\n",
    "# DRLearner model instance - Using correct parameters: model_propensity and model_regression\n",
    "dr_learner = DRLearner(\n",
    "    model_final=model_final,\n",
    "    model_propensity=model_propensity,  # Corrected from 'model_t'\n",
    "    model_regression=model_regression,  # Added for complete doubly robust estimation\n",
    "    cv=5,\n",
    "    mc_iters=3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Training DRLearner on training set...\")\n",
    "# Fit the model using the training data\n",
    "dr_learner.fit(\n",
    "    Y=y_train,\n",
    "    T=treatment_train,\n",
    "    X=X_train,\n",
    "    W=None,  # Assuming no features W that only affect the outcome, not the treatment effect\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. CATE Prediction (Uplift Score) - Using Test Data\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Predict CATE on the held-out TEST set\n",
    "uplift_test_dr = dr_learner.effect(X_test)\n",
    "test_qini_dr = qini_auc_score(y_test, uplift_test_dr, treatment_test)\n",
    "uplift_train_dr = dr_learner.effect(X_train)\n",
    "train_qini_dr = qini_auc_score(y_train, uplift_train_dr, treatment_train)\n",
    "\n",
    "\n",
    "# Add results to the test DataFrame\n",
    "df_results_test = df_test.copy()\n",
    "df_results_test[\"prioritization_score\"] = uplift_test_dr\n",
    "\n",
    "# Sort and Rank the test members\n",
    "df_results_test = df_results_test.sort_values(\n",
    "    by=\"prioritization_score\", ascending=True\n",
    ").reset_index(drop=True)\n",
    "df_results_test[\"rank\"] = df_results_test.index + 1\n",
    "\n",
    "# Output the required ranked list\n",
    "top_n_members_test = df_results_test[[\"member_id\", \"prioritization_score\", \"rank\"]]\n",
    "print(\"\\nTop 5 Ranked Members (Test Set):\")\n",
    "print(top_n_members_test.head())\n",
    "top_n_members_test.to_csv(\"ranked_members_for_outreach_test.csv\", index=False)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Qini/AUUC Calculation and Plotting (Using Test Data)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Calculate and plot the results\n",
    "df_qini_test, auuc_score_test, total_uplift_gain_test = calculate_qini_auuc(\n",
    "    df_results_test\n",
    ")\n",
    "\n",
    "# Plotting the Qini Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    df_qini_test[\"k\"], df_qini_test[\"qini_gain\"], label=\"Model Qini Curve (Uplift)\"\n",
    ")\n",
    "plt.plot(\n",
    "    df_qini_test[\"k\"],\n",
    "    df_qini_test[\"random_gain\"],\n",
    "    linestyle=\"--\",\n",
    "    color=\"gray\",\n",
    "    label=\"Random Targeting Baseline\",\n",
    ")\n",
    "\n",
    "plt.title(f\"Qini Curve (Test Set) - AUUC: {auuc_score_test:.4f}\")\n",
    "plt.xlabel(\"Number of Targeted Members (k)\")\n",
    "plt.ylabel(\"Cumulative Uplift (Net Churn Reduction)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"qini_curve_test.png\")\n",
    "print(f\"\\n--- Uplift Model Metrics (Test Set) ---\")\n",
    "print(f\"Total Theoretical Uplift Gain: {total_uplift_gain_test:.4f}\")\n",
    "print(f\"Area Under the Uplift Curve (AUUC, Normalized): {auuc_score_test:.6f}\")\n",
    "print(\"--- Uplift Model Metrics (Test Set) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uplift Score Variance Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UPLIFT SCORE VARIANCE DIAGNOSTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nUplift score range:\")\n",
    "print(f\"  Min: {uplift_test_dr.min():.6f}\")\n",
    "print(f\"  Max: {uplift_test_dr.max():.6f}\")\n",
    "print(f\"  Range: {uplift_test_dr.max() - uplift_test_dr.min():.6f}\")\n",
    "print(f\"  Mean: {uplift_test_dr.mean():.6f}\")\n",
    "print(f\"  Std: {uplift_test_dr.std():.6f}\")\n",
    "\n",
    "print(f\"   This indicates weak heterogeneous treatment effects\")\n",
    "print(f\"   Model predicts similar uplift for most members\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "plot_qini_curve(\n",
    "    y_train,\n",
    "    uplift_train_dr,\n",
    "    treatment_train,\n",
    "    perfect=True,\n",
    "    name=\"DR-Learner Train\",\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(f\"DR-Learner - Train (Qini AUC: {train_qini_dr:.4f})\")\n",
    "plot_qini_curve(\n",
    "    y_test,\n",
    "    uplift_test_dr,\n",
    "    treatment_test,\n",
    "    perfect=True,\n",
    "    name=\"DR-Learner Test\",\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(f\"DR-Learner - Test (Qini AUC: {test_qini_dr:.4f})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4e901",
   "metadata": {},
   "source": [
    "### t learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f3d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklift.models import TwoModels\n",
    "\n",
    "# Create SEPARATE instances for treatment and control to avoid sklearn error\n",
    "lgbm_t_treatment = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=100,  # Reduced from 300\n",
    "    max_depth=3,  # Shallow trees to prevent overfitting\n",
    "    learning_rate=0.1,  # Increased from 0.03\n",
    "    min_child_samples=100,  # Increased from 30\n",
    "    reg_alpha=0.5,  # Strong L1 regularization\n",
    "    reg_lambda=0.5,  # Strong L2 regularization\n",
    "    min_split_gain=0.1,  # Require minimum gain to split\n",
    "    subsample=0.7,  # Bagging\n",
    "    colsample_bytree=0.7,  # Feature sampling\n",
    ")\n",
    "\n",
    "lgbm_t_control = LGBMClassifier(\n",
    "    random_state=43,  # Different seed\n",
    "    n_jobs=-1,\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    min_child_samples=100,\n",
    "    reg_alpha=10.0,\n",
    "    reg_lambda=5.0,\n",
    "    min_split_gain=0.1,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    ")\n",
    "\n",
    "tlearner = TwoModels(\n",
    "    estimator_trmnt=lgbm_t_treatment, estimator_ctrl=lgbm_t_control, method=\"vanilla\"\n",
    ")\n",
    "\n",
    "tlearner.fit(X_train, y_train, treatment=treatment_train)\n",
    "\n",
    "# Evaluate\n",
    "uplift_train_tlearner = tlearner.predict(X_train)\n",
    "uplift_test_tlearner = tlearner.predict(X_test)\n",
    "\n",
    "train_qini_tlearner = qini_auc_score(y_train, uplift_train_tlearner, treatment_train)\n",
    "test_qini_tlearner = qini_auc_score(y_test, uplift_test_tlearner, treatment_test)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"T-LEARNER (REGULARIZED) WITH LIGHTGBM\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Train Qini AUC: {train_qini_tlearner:.4f}\")\n",
    "print(f\"Test Qini AUC:  {test_qini_tlearner:.4f}\")\n",
    "print(f\"Overfitting Gap: {train_qini_tlearner - test_qini_tlearner:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "plot_qini_curve(\n",
    "    y_train,\n",
    "    uplift_train_tlearner,\n",
    "    treatment_train,\n",
    "    perfect=True,\n",
    "    name=\"T-Learner Train\",\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(f\"T-Learner - Train (Qini AUC: {train_qini_tlearner:.4f})\")\n",
    "plot_qini_curve(\n",
    "    y_test,\n",
    "    uplift_test_tlearner,\n",
    "    treatment_test,\n",
    "    perfect=True,\n",
    "    name=\"T-Learner Test\",\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(f\"T-Learner - Test (Qini AUC: {test_qini_tlearner:.4f})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc1830",
   "metadata": {},
   "source": [
    "### X learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment 2: X-Learner Implementation\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"X-LEARNER (ADVANCED META-LEARNER)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nX-Learner steps:\")\n",
    "print(\"  1. Train models on treatment/control groups (like T-Learner)\")\n",
    "print(\"  2. Impute counterfactual outcomes\")\n",
    "print(\"  3. Train models to predict treatment effects directly\")\n",
    "print(\"  4. Weight by propensity scores\")\n",
    "\n",
    "# Step 1: Train base models on treatment and control\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Models for stage 1\n",
    "mu0_model = Ridge(alpha=10.0, random_state=42)  # Control group model\n",
    "mu1_model = Ridge(alpha=10.0, random_state=43)  # Treatment group model\n",
    "\n",
    "# Split by treatment\n",
    "X_train_treated = X_train[treatment_train == 1]\n",
    "y_train_treated = y_train[treatment_train == 1]\n",
    "X_train_control = X_train[treatment_train == 0]\n",
    "y_train_control = y_train[treatment_train == 0]\n",
    "\n",
    "print(f\"\\nTraining data split:\")\n",
    "print(f\"  Treatment group: {len(X_train_treated)} samples\")\n",
    "print(f\"  Control group: {len(X_train_control)} samples\")\n",
    "\n",
    "# Fit stage 1 models\n",
    "mu0_model.fit(X_train_control, y_train_control)\n",
    "mu1_model.fit(X_train_treated, y_train_treated)\n",
    "\n",
    "# Step 2: Impute counterfactuals\n",
    "# For treated: D_1 = Y_1 - mu_0(X_1)\n",
    "# For control: D_0 = mu_1(X_0) - Y_0\n",
    "\n",
    "D1 = y_train_treated.values - mu0_model.predict(X_train_treated)  # Treated ITE\n",
    "D0 = mu1_model.predict(X_train_control) - y_train_control.values  # Control ITE\n",
    "\n",
    "print(f\"\\nImputed treatment effects:\")\n",
    "print(f\"  Treated group ITE mean: {D1.mean():.6f}\")\n",
    "print(f\"  Control group ITE mean: {D0.mean():.6f}\")\n",
    "\n",
    "# Step 3: Train models to predict treatment effects\n",
    "tau0_model = Ridge(alpha=10.0, random_state=44)  # ITE model on control\n",
    "tau1_model = Ridge(alpha=10.0, random_state=45)  # ITE model on treatment\n",
    "\n",
    "tau0_model.fit(X_train_control, D0)\n",
    "tau1_model.fit(X_train_treated, D1)\n",
    "\n",
    "# Step 4: Predict with propensity weighting\n",
    "# tau(x) = g(x) * tau_0(x) + (1 - g(x)) * tau_1(x)\n",
    "# where g(x) is propensity score\n",
    "\n",
    "if \"propensity_score\" in X_test.columns:\n",
    "    propensity_test = X_test[\"propensity_score\"].values\n",
    "else:\n",
    "    # Estimate propensity if not available\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    prop_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    prop_model.fit(X_train, treatment_train)\n",
    "    propensity_test = prop_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Final X-Learner prediction\n",
    "tau0_pred = tau0_model.predict(X_test)\n",
    "tau1_pred = tau1_model.predict(X_test)\n",
    "\n",
    "uplift_test_xlearner = propensity_test * tau0_pred + (1 - propensity_test) * tau1_pred\n",
    "\n",
    "# Train set predictions\n",
    "if \"propensity_score\" in X_train.columns:\n",
    "    propensity_train = X_train[\"propensity_score\"].values\n",
    "else:\n",
    "    propensity_train = prop_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "tau0_pred_train = tau0_model.predict(X_train)\n",
    "tau1_pred_train = tau1_model.predict(X_train)\n",
    "uplift_train_xlearner = (\n",
    "    propensity_train * tau0_pred_train + (1 - propensity_train) * tau1_pred_train\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_qini_xlearner = qini_auc_score(y_train, uplift_train_xlearner, treatment_train)\n",
    "test_qini_xlearner = qini_auc_score(y_test, uplift_test_xlearner, treatment_test)\n",
    "\n",
    "print(f\"\\nX-Learner Performance:\")\n",
    "print(f\"  Train Qini AUC: {train_qini_xlearner:.4f}\")\n",
    "print(f\"  Test Qini AUC:  {test_qini_xlearner:.4f}\")\n",
    "print(f\"  Overfitting Gap: {train_qini_xlearner - test_qini_xlearner:.4f}\")\n",
    "print(f\"\\nUplift score statistics (test):\")\n",
    "print(f\"  Range: {uplift_test_xlearner.max() - uplift_test_xlearner.min():.6f}\")\n",
    "print(f\"  Mean: {uplift_test_xlearner.mean():.6f}\")\n",
    "print(f\"  Std: {uplift_test_xlearner.std():.6f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "plot_qini_curve(\n",
    "    y_train,\n",
    "    uplift_train_xlearner,\n",
    "    treatment_train,\n",
    "    perfect=True,\n",
    "    name=\"X-Learner Train\",\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(f\"X-Learner - Train (Qini AUC: {train_qini_xlearner:.4f})\")\n",
    "plot_qini_curve(\n",
    "    y_test,\n",
    "    uplift_test_xlearner,\n",
    "    treatment_test,\n",
    "    perfect=True,\n",
    "    name=\"X-Learner Test\",\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(f\"X-Learner - Test (Qini AUC: {test_qini_xlearner:.4f})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25b7fb",
   "metadata": {},
   "source": [
    "## models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize All Meta-Learners (Qini Curves)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "models_viz = [\n",
    "    (\"S-Learner (Baseline)\", uplift_test_enet, test_qini_enet),\n",
    "    (\"T-Learner\", uplift_test_tlearner, test_qini_tlearner),\n",
    "    (\"X-Learner\", uplift_test_xlearner, test_qini_xlearner),\n",
    "    (\"DR-Learner\", uplift_test_dr, test_qini_dr),\n",
    "]\n",
    "\n",
    "\n",
    "for idx, (name, uplift, qini) in enumerate(models_viz):\n",
    "    if idx < len(axes):\n",
    "        plot_qini_curve(\n",
    "            y_test, uplift, treatment_test, perfect=True, name=name, ax=axes[idx]\n",
    "        )\n",
    "        axes[idx].set_title(\n",
    "            f\"{name}\\nQini AUC: {qini:.4f}\", fontweight=\"bold\", fontsize=10\n",
    "        )\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(models_viz), len(axes)):\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    repo_root / \"outputs\" / f\"meta_learner_qini_comparison_{timestamp}.png\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"‚úÖ Qini comparison saved to: outputs/meta_learner_qini_comparison_{timestamp}.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uplift Distribution Comparison\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, uplift, _) in enumerate(models_viz):\n",
    "    if idx < len(axes):\n",
    "        axes[idx].hist(uplift, bins=50, edgecolor=\"black\", alpha=0.7, color=\"steelblue\")\n",
    "        axes[idx].axvline(\n",
    "            uplift.mean(),\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"Mean: {uplift.mean():.5f}\",\n",
    "        )\n",
    "        axes[idx].set_xlabel(\"Uplift Score\")\n",
    "        axes[idx].set_ylabel(\"Frequency\")\n",
    "        axes[idx].set_title(\n",
    "            f\"{name}\\nRange: {uplift.max() - uplift.min():.5f}\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "        axes[idx].legend(fontsize=8)\n",
    "        axes[idx].grid(alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(models_viz), len(axes)):\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    repo_root / \"outputs\" / f\"meta_learner_distributions_{timestamp}.png\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"‚úÖ Distribution comparison saved to: outputs/meta_learner_distributions_{timestamp}.png\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
