{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7327c39",
   "metadata": {},
   "source": [
    "# main nootbook to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b71ab21",
   "metadata": {},
   "source": [
    "## installaions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5331b1",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4590d282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elad\\Desktop\\rnd\\VI\\wellco_churn\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedShuffleSplit,\n",
    "    GridSearchCV,\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    cross_val_score,\n",
    ")\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklift.models import SoloModel\n",
    "\n",
    "# from sklift.viz import plot_qini_curve\n",
    "from sklift.datasets import fetch_megafon\n",
    "from sklift.metrics import make_uplift_scorer\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import re\n",
    "from sklift.metrics import qini_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "from sklift.metrics import qini_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada20e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "repo_root = ([cwd] + list(cwd.parents))[1]\n",
    "\n",
    "# Ensure repo_root is on sys.path so `src.train` can be imported\n",
    "sys.path.append(str(repo_root))\n",
    "from src.process_datasets import create_data, get_web_feats\n",
    "\n",
    "# Load the YAML config file\n",
    "with open(os.path.join(repo_root, \"config.yaml\"), \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a421e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c90ebbf",
   "metadata": {},
   "source": [
    "## data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9a57d",
   "metadata": {},
   "source": [
    "### data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a83dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Load data\n",
    "# ----------------------------\n",
    "app_usage = pd.read_csv(repo_root / \"data\" / \"train\" / \"app_usage.csv\")\n",
    "web_visits = pd.read_csv(repo_root / \"data\" / \"train\" / \"web_visits.csv\")\n",
    "claims = pd.read_csv(repo_root / \"data\" / \"train\" / \"claims.csv\")\n",
    "churn_labels = pd.read_csv(repo_root / \"data\" / \"train\" / \"churn_labels.csv\")\n",
    "test_app_usage = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_app_usage.csv\")\n",
    "test_web_visits = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_web_visits.csv\")\n",
    "test_claims = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_claims.csv\")\n",
    "test_churn_labels = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_churn_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c43bea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train, y_train, treatment_train \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m(\n\u001b[0;32m      2\u001b[0m     app_usage, web_visits, claims, churn_labels, day_first_web\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m x_test, y_test, treatment_test \u001b[38;5;241m=\u001b[39m get_data(\n\u001b[0;32m      6\u001b[0m     test_app_usage, test_web_visits, test_claims, test_churn_labels, day_first_web\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, y_train, treatment_train = get_data(\n",
    "    app_usage, web_visits, claims, churn_labels, day_first_web=True\n",
    ")\n",
    "\n",
    "x_test, y_test, treatment_test = get_data(\n",
    "    test_app_usage, test_web_visits, test_claims, test_churn_labels, day_first_web=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b926afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.visit_trend.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135e98d",
   "metadata": {},
   "source": [
    "### data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a8a9a",
   "metadata": {},
   "source": [
    "### feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c7a27",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589085e",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be44f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce89a219",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59fc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8b4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8010d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b41fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41517044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.fillna(0)\n",
    "# x_test = x_test.fillna(0)\n",
    "# y_train = y_train.fillna(0)\n",
    "# y_test = y_test.fillna(0)\n",
    "# treatment_train = treatment_train.fillna(0)\n",
    "# treatment_test = treatment_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overfitting Diagnostic Dashboard\n",
    "\n",
    "\n",
    "\n",
    "def compare_train_test_performance(\n",
    "    model, X_tr, X_te, y_tr, y_te, t_tr, t_te, model_name=\"Model\"\n",
    "):\n",
    "    \"\"\"Compare model performance on train vs test to detect overfitting\"\"\"\n",
    "\n",
    "    # Predict on both sets\n",
    "    uplift_train = model.predict(X_tr)\n",
    "    uplift_test = model.predict(X_te)\n",
    "\n",
    "    # Calculate Qini AUC\n",
    "    qini_train = qini_auc_score(y_tr, uplift_train, t_tr)\n",
    "    qini_test = qini_auc_score(y_te, uplift_test, t_te)\n",
    "\n",
    "    # Plot side-by-side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    from sklift.viz import plot_qini_curve\n",
    "\n",
    "    plot_qini_curve(y_tr, uplift_train, t_tr, perfect=True, name=\"Train\", ax=ax1)\n",
    "    ax1.set_title(f\"{model_name} - Train (Qini AUC: {qini_train:.4f})\")\n",
    "\n",
    "    plot_qini_curve(y_te, uplift_test, t_te, perfect=True, name=\"Test\", ax=ax2)\n",
    "    ax2.set_title(f\"{model_name} - Test (Qini AUC: {qini_test:.4f})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate overfitting gap\n",
    "    gap = qini_train - qini_test\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Train Qini AUC: {qini_train:.4f}\")\n",
    "    print(f\"Test Qini AUC:  {qini_test:.4f}\")\n",
    "    print(f\"Overfitting Gap: {gap:.4f} ({gap/qini_train*100:.1f}% relative)\")\n",
    "\n",
    "    if gap > 0.01:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Significant overfitting detected!\")\n",
    "    elif gap > 0.005:\n",
    "        print(\"‚ö†Ô∏è  Mild overfitting detected\")\n",
    "    else:\n",
    "        print(\"‚úÖ Good generalization\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return qini_train, qini_test, gap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0522691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {\n",
    "    \"estimator__learning_rate\": [0.01, 0.05],  # Lower learning rates\n",
    "    \"estimator__max_depth\": [2, 3, 4],  # Much shallower trees\n",
    "    \"estimator__min_child_samples\": [100],  # More samples per leaf\n",
    "    \"estimator__reg_alpha\": [5.0],  # Stronger L1 regularization\n",
    "    \"estimator__reg_lambda\": [5.0],  # Stronger L2 regularization\n",
    "    \"estimator__num_leaves\": [7, 15],  # Fewer leaves\n",
    "    \"estimator__subsample\": [0.7, 0.8],  # Add row subsampling\n",
    "    \"estimator__colsample_bytree\": [0.7, 0.8],  # Add column subsampling\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    cv_scores = []\n",
    "    # ParameterGrid returns namespaced keys (estimator__*) - we pass them to the estimator\n",
    "    for train_idx, val_idx in cv.split(x_train, y_train):\n",
    "        X_tr, X_val = x_train.iloc[train_idx], x_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        t_tr, t_val = treatment_train.iloc[train_idx], treatment_train.iloc[val_idx]\n",
    "\n",
    "        # create a fresh model for each fold and parameter setting\n",
    "        est_kwargs = {k.replace(\"estimator__\", \"\"): v for k, v in params.items()}\n",
    "        model = SoloModel(\n",
    "            estimator=LGBMClassifier(\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                class_weight=\"balanced\",\n",
    "                **est_kwargs,\n",
    "                verbose=-1,\n",
    "            )\n",
    "        )\n",
    "        model.fit(X_tr, y_tr, treatment=t_tr)\n",
    "        uplift_val = model.predict(X_val)\n",
    "        q = qini_auc_score(y_val, uplift_val, t_val)\n",
    "        print(q)\n",
    "        cv_scores.append(q)\n",
    "\n",
    "    mean_qini = np.mean(cv_scores)\n",
    "    print(f\"Params: {params} | CV Qini AUC: {mean_qini:.4f}\")\n",
    "    if mean_qini > best_score:\n",
    "        best_score = mean_qini\n",
    "        best_params = params\n",
    "\n",
    "print(\"‚úÖ Best Params:\", best_params)\n",
    "print(\"‚úÖ Best CV Qini AUC:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42057d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e6cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da18bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6a55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1365244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Final Model with Best Regularization\n",
    "\n",
    "# Use best params from grid search\n",
    "final_model = SoloModel(\n",
    "    estimator=LGBMClassifier(random_state=42, n_jobs=-1,is_unbalanced=True)\n",
    ")\n",
    "# final_model.set_params(**best_params)\n",
    "\n",
    "# Train on full training set\n",
    "final_model.fit(x_train, y_train, treatment=treatment_train)\n",
    "\n",
    "# Comprehensive evaluation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "compare_train_test_performance(\n",
    "    final_model,\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    treatment_train,\n",
    "    treatment_test,\n",
    "    model_name=\"Final Regularized S-Learner\",\n",
    ")\n",
    "\n",
    "# Save model if generalization is acceptable\n",
    "uplift_train_final = final_model.predict(x_train)\n",
    "uplift_test_final = final_model.predict(x_test)\n",
    "\n",
    "qini_train_final = qini_auc_score(y_train, uplift_train_final, treatment_train)\n",
    "qini_test_final = qini_auc_score(y_test, uplift_test_final, treatment_test)\n",
    "\n",
    "if (qini_train_final - qini_test_final) < 0.015:  # Accept <0.015 gap\n",
    "    print(\"\\n‚úÖ Model generalization acceptable - saving model\")\n",
    "    import joblib\n",
    "\n",
    "    model_path = repo_root / \"models\" / f\"uplift_slearner_{timestamp}.pkl\"\n",
    "    joblib.dump(final_model, model_path)\n",
    "    print(f\"Saved to: {model_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Model still overfitting - consider simpler model or more data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklift.models.two_models import TwoModel\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 1. Define the base estimator (used for both T=1 and T=0)\n",
    "estimator = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    # REMINDER: Remove class_weight=\"balanced\" if you used it before\n",
    ")\n",
    "\n",
    "# 2. Instantiate the T-Learner wrapper\n",
    "# It creates two copies of the estimator internally: one for T=1 and one for T=0\n",
    "tm = TwoModel(estimator=estimator)\n",
    "\n",
    "# 3. Fit the model using your data\n",
    "tm.fit(X_train, y_train, treatment_train)\n",
    "\n",
    "# 4. Predict the uplift\n",
    "uplift_preds = tm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594696b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Final Model with Best Regularization\n",
    "\n",
    "# Use best params from grid search\n",
    "final_model = SoloModel(\n",
    "    estimator=LGBMClassifier(random_state=42, n_jobs=-1)\n",
    ")\n",
    "final_model.set_params(**best_params)\n",
    "\n",
    "# Train on full training set\n",
    "final_model.fit(x_train, y_train, treatment=treatment_train)\n",
    "\n",
    "# Comprehensive evaluation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "compare_train_test_performance(\n",
    "    final_model,\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    treatment_train,\n",
    "    treatment_test,\n",
    "    model_name=\"Final Regularized S-Learner\",\n",
    ")\n",
    "\n",
    "# Save model if generalization is acceptable\n",
    "uplift_train_final = final_model.predict(x_train)\n",
    "uplift_test_final = final_model.predict(x_test)\n",
    "\n",
    "qini_train_final = qini_auc_score(y_train, uplift_train_final, treatment_train)\n",
    "qini_test_final = qini_auc_score(y_test, uplift_test_final, treatment_test)\n",
    "\n",
    "if (qini_train_final - qini_test_final) < 0.015:  # Accept <0.015 gap\n",
    "    print(\"\\n‚úÖ Model generalization acceptable - saving model\")\n",
    "    import joblib\n",
    "\n",
    "    model_path = repo_root / \"models\" / f\"uplift_slearner_{timestamp}.pkl\"\n",
    "    joblib.dump(final_model, model_path)\n",
    "    print(f\"Saved to: {model_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Model still overfitting - consider simpler model or more data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc371597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install causalml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.inference.meta.drlearner import BaseDRLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378376e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "from causalml.inference.meta import BaseDRLearner\n",
    "# from causalml.metrics import qini_auc_score\n",
    "import joblib\n",
    "\n",
    "# ----------------------------\n",
    "# 0Ô∏è‚É£ Setup\n",
    "# ----------------------------\n",
    "obs_start = pd.to_datetime(\"2025-07-01\")\n",
    "obs_end = pd.to_datetime(\"2025-07-14\")\n",
    "\n",
    "# Ensure all member IDs are in features\n",
    "all_member_ids = churn_labels[\"member_id\"]\n",
    "\n",
    "# ----------------------------\n",
    "# 1Ô∏è‚É£ App usage features\n",
    "# ----------------------------\n",
    "app_total_events = (\n",
    "    app_usage.groupby(\"member_id\")\n",
    "    .size()\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    "    .rename(\"app_total_events\")\n",
    ")\n",
    "app_last_ts = (\n",
    "    app_usage.groupby(\"member_id\")[\"timestamp\"]\n",
    "    .max()\n",
    "    .reindex(all_member_ids, fill_value=obs_start)\n",
    "    .rename(\"app_last_ts\")\n",
    ")\n",
    "\n",
    "# Weekly trend\n",
    "app_usage[\"week\"] = app_usage[\"timestamp\"].dt.isocalendar().week\n",
    "weekly_counts = app_usage.groupby([\"member_id\", \"week\"]).size().unstack(fill_value=0)\n",
    "weekly_counts = weekly_counts.reindex(all_member_ids, fill_value=0)\n",
    "if weekly_counts.shape[1] >= 2:\n",
    "    weekly_counts[\"app_trend\"] = weekly_counts.iloc[:, -1] - weekly_counts.iloc[:, 0]\n",
    "else:\n",
    "    weekly_counts[\"app_trend\"] = 0\n",
    "\n",
    "# ----------------------------\n",
    "# 2Ô∏è‚É£ Web visits features\n",
    "# ----------------------------\n",
    "web_visits[[\"domain\", \"category\", \"page\"]] = web_visits[\"url\"].str.extract(\n",
    "    r\"https?://([^/]+)/([^/]+)/(.+)\"\n",
    ")\n",
    "\n",
    "web_total = (\n",
    "    web_visits.groupby(\"member_id\")\n",
    "    .size()\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    "    .rename(\"web_total\")\n",
    ")\n",
    "web_chronic = (\n",
    "    web_visits[web_visits[\"category\"] == \"chronic\"]\n",
    "    .groupby(\"member_id\")\n",
    "    .size()\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    "    .rename(\"web_chronic\")\n",
    ")\n",
    "web_last_ts = (\n",
    "    web_visits.groupby(\"member_id\")[\"timestamp\"]\n",
    "    .max()\n",
    "    .reindex(all_member_ids, fill_value=obs_start)\n",
    "    .rename(\"web_last_ts\")\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3Ô∏è‚É£ Claims features\n",
    "# ----------------------------\n",
    "claims_count = (\n",
    "    claims.groupby(\"member_id\")\n",
    "    .size()\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    "    .rename(\"claims_count\")\n",
    ")\n",
    "icd_prefix = claims[\"icd_code\"].str[:3]\n",
    "claims_icd_counts = (\n",
    "    claims.groupby([\"member_id\", icd_prefix]).size().unstack(fill_value=0)\n",
    ")\n",
    "claims_icd_counts = claims_icd_counts.reindex(all_member_ids, fill_value=0)\n",
    "\n",
    "# ----------------------------\n",
    "# 4Ô∏è‚É£ Signup / recency features\n",
    "# ----------------------------\n",
    "signup_date = pd.to_datetime(\n",
    "    churn_labels.set_index(\"member_id\")[\"signup_date\"]\n",
    ").reindex(all_member_ids)\n",
    "days_since_signup = (obs_end - signup_date).dt.days.rename(\"days_since_signup\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5Ô∏è‚É£ Merge all features\n",
    "# ----------------------------\n",
    "features = pd.concat(\n",
    "    [\n",
    "        app_total_events,\n",
    "        app_last_ts,\n",
    "        weekly_counts[\"app_trend\"],\n",
    "        web_total,\n",
    "        web_chronic,\n",
    "        web_last_ts,\n",
    "        claims_count,\n",
    "        claims_icd_counts,\n",
    "        days_since_signup,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Convert timestamps to numeric\n",
    "for col in [\"app_last_ts\", \"web_last_ts\"]:\n",
    "    features[col] = (features[col] - obs_start).dt.total_seconds()\n",
    "\n",
    "features = features.fillna(0)\n",
    "\n",
    "# ----------------------------\n",
    "# 6Ô∏è‚É£ Build propensity features\n",
    "# ----------------------------\n",
    "treatment_aligned = treatment_train.reindex(features.index).fillna(0)\n",
    "prop_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "prop_model.fit(features, treatment_aligned.values)\n",
    "propensity_score = pd.Series(\n",
    "    prop_model.predict_proba(features)[:, 1],\n",
    "    index=features.index,\n",
    "    name=\"propensity_score\",\n",
    ")\n",
    "\n",
    "features[\"propensity_score\"] = propensity_score\n",
    "\n",
    "# ----------------------------\n",
    "# 7Ô∏è‚É£ Align train/test by member_id\n",
    "# ----------------------------\n",
    "x_train_final = features.loc[x_train[\"member_id\"]].fillna(0)\n",
    "x_test_final = features.loc[x_test[\"member_id\"]].fillna(0)\n",
    "\n",
    "\n",
    "y_train_aligned = y_train.reindex(x_train_final.index).fillna(0).astype(\"int64\")\n",
    "\n",
    "y_test_aligned = y_test.reindex(x_test_final.index).fillna(0).astype(\"int64\")\n",
    "\n",
    "treatment_train_aligned = (\n",
    "    treatment_train.reindex(x_train_final.index).fillna(0).astype(\"int64\")\n",
    ")\n",
    "\n",
    "treatment_test_aligned = (\n",
    "    treatment_test.reindex(x_test_final.index).fillna(0).astype(\"int64\")\n",
    ")\n",
    "# ----------------------------\n",
    "# 8Ô∏è‚É£ Train DR-Learner\n",
    "# ----------------------------\n",
    "dr_model = BaseDRLearner(learner=LGBMRegressor(random_state=42, n_jobs=-1))\n",
    "dr_model.fit(\n",
    "    X=x_train_final.values,\n",
    "    treatment=treatment_train_aligned.values,\n",
    "    y=y_train_aligned.values,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 9Ô∏è‚É£ Predict uplift\n",
    "# ----------------------------\n",
    "uplift_train_final = dr_model.predict(x_train_final.values)\n",
    "uplift_test_final = dr_model.predict(x_test_final.values)\n",
    "\n",
    "# ----------------------------\n",
    "# üîü Qini evaluation\n",
    "# ----------------------------\n",
    "qini_train_final = qini_auc_score(\n",
    "    y_train_aligned.values, uplift_train_final, treatment_train_aligned.values\n",
    ")\n",
    "qini_test_final = qini_auc_score(\n",
    "    y_test_aligned.values, uplift_test_final, treatment_test_aligned.values\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL DR-LEARNER MODEL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Qini AUC - Train: {qini_train_final:.4f}\")\n",
    "print(f\"Qini AUC - Test:  {qini_test_final:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Save model if generalization acceptable\n",
    "# ----------------------------\n",
    "gap = qini_train_final - qini_test_final\n",
    "if gap < 0.015:\n",
    "    print(\"\\n‚úÖ Model generalization acceptable - saving model\")\n",
    "    model_path = repo_root / \"models\" / f\"uplift_drlearner_{timestamp}.pkl\"\n",
    "    joblib.dump(dr_model, model_path)\n",
    "    print(f\"Saved to: {model_path}\")\n",
    "else:\n",
    "    print(\n",
    "        \"\\n‚ö†Ô∏è WARNING: Model still overfitting or signal weak - consider simpler learner or more features\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fba8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.head())\n",
    "print(treatment_train.mean())\n",
    "print(y_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8f67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee222f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from causalml.inference.meta import BaseDRLearner\n",
    "from lightgbm import LGBMClassifier\n",
    "# from causalml.metrics import qini_auc_score\n",
    "import joblib\n",
    "from lightgbm import LGBMRegressor\n",
    "from causalml.inference.meta import BaseDRLearner\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Ensure correct data types\n",
    "# ----------------------------\n",
    "y_train = y_train.astype(\"int64\")\n",
    "y_test = y_test.astype(\"int64\")\n",
    "treatment_train = treatment_train.astype(\"int64\")\n",
    "treatment_test = treatment_test.astype(\"int64\")\n",
    "\n",
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "# ----------------------------\n",
    "# Create DR Learner\n",
    "# ----------------------------\n",
    "# Optional: use best_params from grid search\n",
    "# estimator_params = {k.replace(\"estimator__\", \"\"): v for k, v in best_params.items()}\n",
    "dr_model = BaseDRLearner(learner=LGBMRegressor(random_state=42, n_jobs=-1))\n",
    "# ----------------------------\n",
    "# Train on full training set\n",
    "# ----------------------------\n",
    "dr_model.fit(X=x_train.values, treatment=treatment_train.values, y=y_train.values)\n",
    "\n",
    "# ----------------------------\n",
    "# Predictions\n",
    "# ----------------------------\n",
    "uplift_train_final = dr_model.predict(X=x_train.values)\n",
    "uplift_test_final = dr_model.predict(X=x_test.values)\n",
    "\n",
    "# ----------------------------\n",
    "# Qini AUC evaluation\n",
    "# ----------------------------\n",
    "qini_train_final = qini_auc_score(\n",
    "    y_train.values, uplift_train_final, treatment_train.values\n",
    ")\n",
    "qini_test_final = qini_auc_score(\n",
    "    y_test.values, uplift_test_final, treatment_test.values\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL DR-LEARNER MODEL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Qini AUC - Train: {qini_train_final:.4f}\")\n",
    "print(f\"Qini AUC - Test:  {qini_test_final:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Save model if generalization is acceptable\n",
    "# ----------------------------\n",
    "gap = qini_train_final - qini_test_final\n",
    "if gap < 0.015:  # Acceptable generalization\n",
    "    print(\"\\n‚úÖ Model generalization acceptable - saving model\")\n",
    "    model_path = repo_root / \"models\" / f\"uplift_drlearner_{timestamp}.pkl\"\n",
    "    joblib.dump(dr_model, model_path)\n",
    "    print(f\"Saved to: {model_path}\")\n",
    "else:\n",
    "    print(\n",
    "        \"\\n‚ö†Ô∏è WARNING: Model still overfitting - consider simpler estimator or more data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from causalml.inference.meta import BaseDRLearner\n",
    "# from causalml.metrics import qini_auc_score\n",
    "import joblib\n",
    "\n",
    "# ----------------------------\n",
    "# Settings\n",
    "# ----------------------------\n",
    "obs_start = pd.to_datetime(\"2025-07-01\")\n",
    "obs_end = pd.to_datetime(\"2025-07-14\")\n",
    "SAVE_MODEL = False  # set True to save model\n",
    "MODEL_PATH = \"uplift_drlearner_final.pkl\"\n",
    "\n",
    "# ----------------------------\n",
    "# Ensure training labels & treatment are correct dtypes\n",
    "# ----------------------------\n",
    "y_train = y_train.astype(\"int64\")\n",
    "y_test = y_test.astype(\"int64\")\n",
    "treatment_train = treatment_train.astype(\"int64\")\n",
    "treatment_test = treatment_test.astype(\"int64\")\n",
    "\n",
    "# ----------------------------\n",
    "# all_member_ids: use churn_labels as canonical list of members\n",
    "# ----------------------------\n",
    "all_member_ids = churn_labels[\"member_id\"]\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Build base feature table (indexed by member_id)\n",
    "# ----------------------------\n",
    "# a) app usage aggregates\n",
    "app_usage[\"timestamp\"] = pd.to_datetime(app_usage[\"timestamp\"], errors=\"coerce\")\n",
    "app_total_events = (\n",
    "    app_usage.groupby(\"member_id\")\n",
    "    .size()\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    "    .rename(\"app_total_events\")\n",
    ")\n",
    "\n",
    "# 'app_last_ts' is custom numeric encoding (higher == more recent/active).\n",
    "# If app_last_ts exists in your precomputed features table, use it; else compute from app_usage.\n",
    "if \"app_last_ts\" in app_usage.columns:\n",
    "    # If present in app_usage rows (unlikely), else we'll compute from groupby.\n",
    "    pass\n",
    "\n",
    "# Compute numeric encoding if not already present in a features DF:\n",
    "# For group-by we will take the max of the custom value if present, otherwise fallback to timestamp seconds.\n",
    "if \"app_last_ts\" in app_usage.columns and app_usage[\"app_last_ts\"].notna().any():\n",
    "    app_last_raw = (\n",
    "        app_usage.groupby(\"member_id\")[\"app_last_ts\"]\n",
    "        .max()\n",
    "        .reindex(all_member_ids, fill_value=0)\n",
    "        .rename(\"app_last_raw\")\n",
    "    )\n",
    "else:\n",
    "    # fallback: use timestamp seconds since obs_start as proxy encoding\n",
    "    app_last_raw = (\n",
    "        app_usage.groupby(\"member_id\")[\"timestamp\"]\n",
    "        .max()\n",
    "        .reindex(all_member_ids, fill_value=pd.NaT)\n",
    "        .fillna(obs_start)\n",
    "        .apply(lambda ts: int((ts - obs_start).total_seconds()))\n",
    "        .rename(\"app_last_raw\")\n",
    "    )\n",
    "\n",
    "# b) web visits aggregates\n",
    "web_visits[\"timestamp\"] = pd.to_datetime(web_visits[\"timestamp\"], errors=\"coerce\")\n",
    "# Extract domain/category/page robustly\n",
    "web_visits[[\"domain\", \"category\", \"page\"]] = web_visits[\"url\"].str.extract(\n",
    "    r\"https?://([^/]+)/([^/]+)/(.+)\", expand=True\n",
    ")\n",
    "web_total = (\n",
    "    web_visits.groupby(\"member_id\")\n",
    "    .size()\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    "    .rename(\"web_total\")\n",
    ")\n",
    "\n",
    "# web_last_raw as custom numeric if exists else from timestamp\n",
    "if \"web_last_ts\" in web_visits.columns and web_visits[\"web_last_ts\"].notna().any():\n",
    "    web_last_raw = (\n",
    "        web_visits.groupby(\"member_id\")[\"web_last_ts\"]\n",
    "        .max()\n",
    "        .reindex(all_member_ids, fill_value=0)\n",
    "        .rename(\"web_last_raw\")\n",
    "    )\n",
    "else:\n",
    "    web_last_raw = (\n",
    "        web_visits.groupby(\"member_id\")[\"timestamp\"]\n",
    "        .max()\n",
    "        .reindex(all_member_ids, fill_value=pd.NaT)\n",
    "        .fillna(obs_start)\n",
    "        .apply(lambda ts: int((ts - obs_start).total_seconds()))\n",
    "        .rename(\"web_last_raw\")\n",
    "    )\n",
    "\n",
    "# web chronic counts and unique domains\n",
    "web_chronic = (\n",
    "    web_visits[web_visits[\"category\"] == \"chronic\"]\n",
    "    .groupby(\"member_id\")\n",
    "    .size()\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    "    .rename(\"web_chronic\")\n",
    ")\n",
    "web_unique_domains = (\n",
    "    web_visits.groupby(\"member_id\")[\"domain\"]\n",
    "    .nunique()\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    "    .rename(\"web_unique_domains\")\n",
    ")\n",
    "\n",
    "# c) claims aggregates\n",
    "claims_count = (\n",
    "    claims.groupby(\"member_id\")\n",
    "    .size()\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    "    .rename(\"claims_count\")\n",
    ")\n",
    "# ICD prefix counts (top prefixes only to avoid huge expansion)\n",
    "claims[\"icd_prefix\"] = claims[\"icd_code\"].astype(str).str[:3]\n",
    "icd_counts = claims.groupby([\"member_id\", \"icd_prefix\"]).size().unstack(fill_value=0)\n",
    "# keep top N prefixes by frequency to avoid explosion\n",
    "top_icd_prefixes = icd_counts.sum().sort_values(ascending=False).head(10).index.tolist()\n",
    "claims_icd_counts = icd_counts[top_icd_prefixes].reindex(all_member_ids, fill_value=0)\n",
    "claims_icd_counts.columns = [f\"icd_{c}\" for c in claims_icd_counts.columns]\n",
    "\n",
    "# d) signup / tenure\n",
    "signup_date = pd.to_datetime(\n",
    "    churn_labels.set_index(\"member_id\")[\"signup_date\"]\n",
    ").reindex(all_member_ids)\n",
    "days_since_signup = (\n",
    "    (obs_end - signup_date)\n",
    "    .dt.days.rename(\"days_since_signup\")\n",
    "    .fillna((obs_end - obs_start).days)\n",
    ")\n",
    "\n",
    "# e) weekly trend features (app)\n",
    "# Build week index 1/2 for the observation window (safe even if weeks overlap calendar weeks)\n",
    "app_usage = app_usage.copy()\n",
    "app_usage[\"day_index\"] = (app_usage[\"timestamp\"] - obs_start).dt.days.clip(\n",
    "    lower=0, upper=(obs_end - obs_start).days\n",
    ")\n",
    "app_usage[\"week_half\"] = np.where(\n",
    "    app_usage[\"day_index\"] <= 6, 1, 2\n",
    ")  # week1: days 0-6, week2: days 7-13\n",
    "app_weekly = (\n",
    "    app_usage.groupby([\"member_id\", \"week_half\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    ")\n",
    "# Ensure columns 1 and 2 exist\n",
    "for c in [1, 2]:\n",
    "    if c not in app_weekly.columns:\n",
    "        app_weekly[c] = 0\n",
    "app_weekly = app_weekly[[1, 2]]\n",
    "app_weekly.columns = [\"app_week1\", \"app_week2\"]\n",
    "app_weekly[\"app_trend\"] = app_weekly[\"app_week2\"] - app_weekly[\"app_week1\"]\n",
    "\n",
    "# f) web weekly trend\n",
    "web_visits = web_visits.copy()\n",
    "web_visits[\"day_index\"] = (web_visits[\"timestamp\"] - obs_start).dt.days.clip(\n",
    "    lower=0, upper=(obs_end - obs_start).days\n",
    ")\n",
    "web_visits[\"week_half\"] = np.where(web_visits[\"day_index\"] <= 6, 1, 2)\n",
    "web_weekly = (\n",
    "    web_visits.groupby([\"member_id\", \"week_half\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .reindex(all_member_ids, fill_value=0)\n",
    ")\n",
    "for c in [1, 2]:\n",
    "    if c not in web_weekly.columns:\n",
    "        web_weekly[c] = 0\n",
    "web_weekly = web_weekly[[1, 2]]\n",
    "web_weekly.columns = [\"web_week1\", \"web_week2\"]\n",
    "web_weekly[\"web_trend\"] = web_weekly[\"web_week2\"] - web_weekly[\"web_week1\"]\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Assemble features DataFrame\n",
    "# ----------------------------\n",
    "features = pd.concat(\n",
    "    [\n",
    "        app_total_events,\n",
    "        app_last_raw,\n",
    "        app_weekly[[\"app_week1\", \"app_week2\", \"app_trend\"]],\n",
    "        web_total,\n",
    "        web_chronic,\n",
    "        web_unique_domains,\n",
    "        web_last_raw,\n",
    "        web_weekly[[\"web_week1\", \"web_week2\", \"web_trend\"]],\n",
    "        claims_count,\n",
    "        claims_icd_counts,\n",
    "        days_since_signup,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Standardize column names in case of raw names\n",
    "features = features.rename(\n",
    "    columns={\"app_last_raw\": \"app_last_raw\", \"web_last_raw\": \"web_last_raw\"}\n",
    ")\n",
    "\n",
    "# Fill any remaining NaNs with 0\n",
    "features = features.fillna(0)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Convert custom \"last activity\" encoding into recency\n",
    "#    We assume higher raw encoding = more recent activity; recency = max_raw - raw\n",
    "# ----------------------------\n",
    "for col in [\"app_last_raw\", \"web_last_raw\"]:\n",
    "    max_val = features[col].max()\n",
    "    # if max_val == 0 then keep zeros; avoid negative recency\n",
    "    if max_val > 0:\n",
    "        features[f\"{col}_recency\"] = (max_val - features[col]).astype(float)\n",
    "    else:\n",
    "        features[f\"{col}_recency\"] = 0.0\n",
    "    # drop raw\n",
    "    features.drop(columns=[col], inplace=True)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Clip extreme values (reduce influence of outliers)\n",
    "# ----------------------------\n",
    "def clip_by_train_quantiles(train_df, test_df, lower_q=0.01, upper_q=0.99):\n",
    "    clipped_train = train_df.copy()\n",
    "    clipped_test = test_df.copy()\n",
    "    for c in train_df.columns:\n",
    "        lo = train_df[c].quantile(lower_q)\n",
    "        hi = train_df[c].quantile(upper_q)\n",
    "        clipped_train[c] = train_df[c].clip(lo, hi)\n",
    "        # use train quantiles to clip test\n",
    "        clipped_test[c] = test_df[c].clip(lo, hi)\n",
    "    return clipped_train, clipped_test\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Add simple interaction features (helpful for non-random treatment)\n",
    "# ----------------------------\n",
    "# Make sure no zeros for multiplication/division issues\n",
    "features[\"total_activity\"] = features[\"app_total_events\"] + features[\"web_total\"] + 1.0\n",
    "features[\"activity_per_day\"] = features[\"total_activity\"] / (\n",
    "    features[\"days_since_signup\"].clip(lower=1)\n",
    ")\n",
    "\n",
    "# temp placeholder for propensity (will be computed next)\n",
    "# ----------------------------\n",
    "# 6) Fit propensity model (use all features except target)\n",
    "# ----------------------------\n",
    "# Align treatment series to features index\n",
    "treatment_aligned = treatment_train.reindex(features.index).fillna(0).astype(\"int64\")\n",
    "# Save columns used for propensity model\n",
    "propensity_feature_cols = features.columns.tolist()\n",
    "\n",
    "# Fit propensity model on these columns\n",
    "prop_model = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=8, random_state=42, n_jobs=-1\n",
    ")\n",
    "prop_model.fit(features[propensity_feature_cols], treatment_aligned.values)\n",
    "\n",
    "# Add propensity score\n",
    "features[\"propensity_score\"] = prop_model.predict_proba(\n",
    "    features[propensity_feature_cols]\n",
    ")[:, 1]\n",
    "\n",
    "\n",
    "# Add interaction terms with propensity and activity\n",
    "features[\"high_activity_low_prop\"] = features[\"total_activity\"] * (\n",
    "    1 - features[\"propensity_score\"]\n",
    ")\n",
    "features[\"low_activity_high_prop\"] = (1.0 / features[\"total_activity\"]) * features[\n",
    "    \"propensity_score\"\n",
    "]\n",
    "\n",
    "# Final fillna just in case\n",
    "features = features.fillna(0)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Align train/test by member_id\n",
    "# ----------------------------\n",
    "# helper to extract member_id index from x_train/x_test\n",
    "def member_index_from_X(X):\n",
    "    # If X has a 'member_id' column use that, else assume its index are member_ids\n",
    "    if \"member_id\" in X.columns:\n",
    "        return pd.Index(X[\"member_id\"].values)\n",
    "    else:\n",
    "        # if X.index is member_id-like (not 0..n), return it\n",
    "        return X.index\n",
    "\n",
    "\n",
    "try:\n",
    "    train_member_ids = member_index_from_X(x_train)\n",
    "    test_member_ids = member_index_from_X(x_test)\n",
    "except Exception:\n",
    "    train_member_ids = x_train.index\n",
    "    test_member_ids = x_test.index\n",
    "\n",
    "# ensure members present; if not present, raise friendly error\n",
    "missing_train = set(train_member_ids) - set(features.index)\n",
    "missing_test = set(test_member_ids) - set(features.index)\n",
    "if missing_train:\n",
    "    print(\n",
    "        f\"Warning: {len(missing_train)} train member_ids not in features (they will be dropped). Example: {list(missing_train)[:5]}\"\n",
    "    )\n",
    "if missing_test:\n",
    "    print(\n",
    "        f\"Warning: {len(missing_test)} test member_ids not in features (they will be dropped). Example: {list(missing_test)[:5]}\"\n",
    "    )\n",
    "\n",
    "# intersect to avoid KeyError\n",
    "train_member_ids = [m for m in train_member_ids if m in features.index]\n",
    "test_member_ids = [m for m in test_member_ids if m in features.index]\n",
    "\n",
    "x_train_final = features.loc[train_member_ids].copy()\n",
    "x_test_final = features.loc[test_member_ids].copy()\n",
    "\n",
    "# Align labels and treatment to these member ids\n",
    "y_train_aligned = y_train.reindex(x_train_final.index).fillna(0).astype(\"int64\")\n",
    "y_test_aligned = y_test.reindex(x_test_final.index).fillna(0).astype(\"int64\")\n",
    "treatment_train_aligned = (\n",
    "    treatment_train.reindex(x_train_final.index).fillna(0).astype(\"int64\")\n",
    ")\n",
    "treatment_test_aligned = (\n",
    "    treatment_test.reindex(x_test_final.index).fillna(0).astype(\"int64\")\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Clip using train quantiles to limit outliers\n",
    "# ----------------------------\n",
    "x_train_clipped, x_test_clipped = clip_by_train_quantiles(\n",
    "    x_train_final, x_test_final, 0.01, 0.99\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Train DR-Learner with LGBMRegressor (learner for continuous pseudo-outcomes)\n",
    "# ----------------------------\n",
    "learner = LGBMRegressor(\n",
    "    max_depth=3,\n",
    "    num_leaves=15,\n",
    "    learning_rate=0.03,\n",
    "    min_child_samples=150,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=5,\n",
    "    reg_lambda=5,\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "dr_model = BaseDRLearner(learner=learner)\n",
    "dr_model.fit(\n",
    "    X=x_train_clipped.values,\n",
    "    treatment=treatment_train_aligned.values,\n",
    "    y=y_train_aligned.values,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Predict uplift and evaluate\n",
    "# ----------------------------\n",
    "uplift_train = dr_model.predict(x_train_clipped.values)\n",
    "uplift_test = dr_model.predict(x_test_clipped.values)\n",
    "\n",
    "qini_train = qini_auc_score(\n",
    "    y_train_aligned.values, uplift_train, treatment_train_aligned.values\n",
    ")\n",
    "qini_test = qini_auc_score(\n",
    "    y_test_aligned.values, uplift_test, treatment_test_aligned.values\n",
    ")\n",
    "\n",
    "print(\"\\nFINAL DR-LEARNER MODEL EVALUATION\")\n",
    "print(f\"Train Qini AUC: {qini_train:.4f}\")\n",
    "print(f\"Test  Qini AUC: {qini_test:.4f}\")\n",
    "print(f\"Treatment rate (train): {treatment_train_aligned.mean():.4f}\")\n",
    "print(f\"Outcome rate (train):   {y_train_aligned.mean():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Optional: save model\n",
    "# ----------------------------\n",
    "if SAVE_MODEL:\n",
    "    joblib.dump(dr_model, MODEL_PATH)\n",
    "    print(f\"Saved model to {MODEL_PATH}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Quick feature importances from propensity model (diagnostic)\n",
    "# ----------------------------\n",
    "feat_imp = pd.Series(\n",
    "    prop_model.feature_importances_, index=propensity_feature_cols\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop propensity features:\")\n",
    "print(feat_imp.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# FULL DR-LEARNER UPLIFT PIPELINE\n",
    "# ================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from lightgbm import LGBMRegressor\n",
    "from causalml.inference.meta import BaseDRLearner\n",
    "# from causalml.metrics import qini_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Paths / constants\n",
    "# ----------------------------\n",
    "# repo_root = Path(\".\")\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load raw datasets\n",
    "# ----------------------------\n",
    "app_usage = pd.read_csv(repo_root / \"data\" / \"train\" / \"app_usage.csv\")\n",
    "web_visits = pd.read_csv(repo_root / \"data\" / \"train\" / \"web_visits.csv\")\n",
    "claims = pd.read_csv(repo_root / \"data\" / \"train\" / \"claims.csv\")\n",
    "churn_labels = pd.read_csv(repo_root / \"data\" / \"train\" / \"churn_labels.csv\")\n",
    "test_app_usage = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_app_usage.csv\")\n",
    "test_web_visits = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_web_visits.csv\")\n",
    "test_claims = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_claims.csv\")\n",
    "test_churn_labels = pd.read_csv(repo_root / \"data\" / \"test\" / \"test_churn_labels.csv\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Feature engineering via get_data()\n",
    "# ----------------------------\n",
    "def get_data(app_usage, web_visits, claims, churn_labels, day_first_web):\n",
    "    # --------- churn labels ---------\n",
    "    churn_labels[\"signup_date\"] = pd.to_datetime(churn_labels[\"signup_date\"])\n",
    "    churn_labels[\"tenure_days\"] = (\n",
    "        pd.Timestamp(\"2025-07-14\") - churn_labels[\"signup_date\"]\n",
    "    ).dt.days\n",
    "    churn_labels[\"treatment\"] = churn_labels[\"outreach\"]\n",
    "    churn_labels[\"y\"] = churn_labels[\"churn\"]\n",
    "\n",
    "    # --------- app_usage features ---------\n",
    "    app_usage[\"timestamp\"] = pd.to_datetime(app_usage[\"timestamp\"])\n",
    "    app_usage_agg = (\n",
    "        app_usage.groupby(\"member_id\")\n",
    "        .agg(\n",
    "            app_sessions=(\"timestamp\", \"count\"),\n",
    "            active_days=(\"timestamp\", lambda x: x.dt.date.nunique()),\n",
    "            first_session=(\"timestamp\", \"min\"),\n",
    "            last_session=(\"timestamp\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    mid_date = pd.Timestamp(\"2025-07-07\")\n",
    "\n",
    "    def usage_trend(group):\n",
    "        early = (group[\"timestamp\"] <= mid_date).sum()\n",
    "        late = (group[\"timestamp\"] > mid_date).sum()\n",
    "        return pd.Series(\n",
    "            {\"early_usage\": early, \"late_usage\": late, \"usage_trend\": late - early}\n",
    "        )\n",
    "\n",
    "    app_trend = app_usage.groupby(\"member_id\").apply(usage_trend).reset_index()\n",
    "    app_features = pd.merge(app_usage_agg, app_trend, on=\"member_id\")\n",
    "\n",
    "    # --------- web_visits features ---------\n",
    "    web_visits[\"timestamp\"] = pd.to_datetime(\n",
    "        web_visits[\"timestamp\"], errors=\"coerce\", dayfirst=day_first_web\n",
    "    )\n",
    "    web_visits[[\"domain\", \"category\", \"page\"]] = web_visits[\"url\"].str.extract(\n",
    "        r\"https://([^/]+)/([^/]+)/(\\d+)\"\n",
    "    )\n",
    "    web_visits[\"is_wellco_domain\"] = web_visits[\"domain\"].str.contains(\n",
    "        \"wellco\", na=False\n",
    "    )\n",
    "\n",
    "    web_agg = (\n",
    "        web_visits.groupby(\"member_id\")\n",
    "        .agg(\n",
    "            total_web_visits=(\"url\", \"count\"),\n",
    "            unique_domains=(\"domain\", \"nunique\"),\n",
    "            unique_categories=(\"category\", \"nunique\"),\n",
    "            unique_pages=(\"page\", \"nunique\"),\n",
    "            last_visit=(\"timestamp\", \"max\"),\n",
    "            wellco_domain_visits=(\"is_wellco_domain\", \"sum\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    web_agg[\"ratio_wellco_domain\"] = (\n",
    "        web_agg[\"wellco_domain_visits\"] / web_agg[\"total_web_visits\"]\n",
    "    )\n",
    "\n",
    "    def web_trend(group):\n",
    "        early = (group[\"timestamp\"] <= mid_date).shape[0]\n",
    "        late = (group[\"timestamp\"] > mid_date).shape[0]\n",
    "        return pd.Series(\n",
    "            {\"early_visits\": early, \"late_visits\": late, \"visit_trend\": late - early}\n",
    "        )\n",
    "\n",
    "    web_trend_df = web_visits.groupby(\"member_id\").apply(web_trend).reset_index()\n",
    "    web_features = pd.merge(web_agg, web_trend_df, on=\"member_id\")\n",
    "\n",
    "    # --------- claims features ---------\n",
    "    claims[\"diagnosis_date\"] = pd.to_datetime(claims[\"diagnosis_date\"])\n",
    "    claims[\"icd_category\"] = claims[\"icd_code\"].str[:3]\n",
    "\n",
    "    priority_icd_codes = [\n",
    "        \"Z71.3\",\n",
    "        \"J00\",\n",
    "        \"M54.5\",\n",
    "        \"I10\",\n",
    "        \"E11.9\",\n",
    "        \"K21.9\",\n",
    "        \"R51\",\n",
    "        \"A09\",\n",
    "        \"B34.9\",\n",
    "        \"H10.9\",\n",
    "    ]\n",
    "    for icd in priority_icd_codes:\n",
    "        claims[f\"has_icd_{icd.replace('.', '_')}\"] = (claims[\"icd_code\"] == icd).astype(\n",
    "            int\n",
    "        )\n",
    "\n",
    "    claims_agg = (\n",
    "        claims.groupby(\"member_id\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"icd_code\": [\"count\", \"nunique\"],\n",
    "                \"icd_category\": \"nunique\",\n",
    "                \"diagnosis_date\": \"max\",\n",
    "                **{\n",
    "                    f\"has_icd_{icd.replace('.', '_')}\": \"max\"\n",
    "                    for icd in priority_icd_codes\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    claims_agg.columns = [\n",
    "        \"member_id\",\n",
    "        \"total_claims\",\n",
    "        \"unique_icd_codes\",\n",
    "        \"unique_icd_categories\",\n",
    "        \"last_claim\",\n",
    "    ] + [f\"has_icd_{icd.replace('.', '_')}\" for icd in priority_icd_codes]\n",
    "\n",
    "    def claims_trend(group):\n",
    "        early = (group[\"diagnosis_date\"] <= mid_date).sum()\n",
    "        late = (group[\"diagnosis_date\"] > mid_date).sum()\n",
    "        return pd.Series(\n",
    "            {\"early_claims\": early, \"late_claims\": late, \"claim_trend\": late - early}\n",
    "        )\n",
    "\n",
    "    claims_trend_df = claims.groupby(\"member_id\").apply(claims_trend).reset_index()\n",
    "    claims_features = pd.merge(claims_agg, claims_trend_df, on=\"member_id\")\n",
    "\n",
    "    # --------- merge all features ---------\n",
    "    dfs = [churn_labels, app_features, web_features, claims_features]\n",
    "    full_features = reduce(\n",
    "        lambda left, right: pd.merge(left, right, on=\"member_id\", how=\"left\"), dfs\n",
    "    )\n",
    "\n",
    "    # Fill NaNs\n",
    "    count_cols = [\n",
    "        c\n",
    "        for c in full_features.columns\n",
    "        if any(k in c for k in [\"count\", \"usage\", \"visits\", \"claims\"])\n",
    "    ]\n",
    "    full_features[count_cols] = full_features[count_cols].fillna(0)\n",
    "    date_cols = [\"first_session\", \"last_session\", \"last_visit\", \"last_claim\"]\n",
    "    for col in date_cols:\n",
    "        full_features[col] = pd.to_datetime(full_features[col])\n",
    "        full_features[col] = full_features[col].fillna(pd.Timestamp(\"2025-07-01\"))\n",
    "\n",
    "    # Final X, y, treatment\n",
    "    exclude_cols = [\n",
    "        \"signup_date\",\n",
    "        \"churn\",\n",
    "        \"outreach\",\n",
    "        \"treatment\",\n",
    "        \"y\",\n",
    "        \"first_session\",\n",
    "        \"last_session\",\n",
    "        \"last_claim\",\n",
    "        \"last_visit\",\n",
    "    ]\n",
    "    feature_cols = [c for c in full_features.columns if c not in exclude_cols]\n",
    "    X = full_features[feature_cols]\n",
    "    y = full_features[\"y\"]\n",
    "    treatment = full_features[\"treatment\"]\n",
    "\n",
    "    return X, y, treatment\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Create train/test features\n",
    "# ----------------------------\n",
    "x_train, y_train, treatment_train = get_data(\n",
    "    app_usage, web_visits, claims, churn_labels, day_first_web=True\n",
    ")\n",
    "x_test, y_test, treatment_test = get_data(\n",
    "    test_app_usage, test_web_visits, test_claims, test_churn_labels, day_first_web=False\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) DR-Learner pipeline\n",
    "# ----------------------------\n",
    "# Fill NaNs\n",
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "# Ensure numeric types\n",
    "y_train = y_train.astype(\"int64\")\n",
    "y_test = y_test.astype(\"int64\")\n",
    "treatment_train = treatment_train.astype(\"int64\")\n",
    "treatment_test = treatment_test.astype(\"int64\")\n",
    "\n",
    "# Propensity score model\n",
    "prop_model = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=8, random_state=42, n_jobs=-1\n",
    ")\n",
    "prop_model.fit(x_train, treatment_train)\n",
    "x_train[\"propensity_score\"] = prop_model.predict_proba(x_train)[:, 1]\n",
    "x_test[\"propensity_score\"] = prop_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Train DR-Learner\n",
    "dr_model = BaseDRLearner(learner=LGBMRegressor(random_state=42, n_jobs=-1))\n",
    "dr_model.fit(X=x_train.values, treatment=treatment_train.values, y=y_train.values)\n",
    "\n",
    "# Predict uplift\n",
    "uplift_train = dr_model.predict(x_train.values)\n",
    "uplift_test = dr_model.predict(x_test.values)\n",
    "\n",
    "# Evaluate\n",
    "qini_train = qini_auc_score(y_train.values, uplift_train, treatment_train.values)\n",
    "qini_test = qini_auc_score(y_test.values, uplift_test, treatment_test.values)\n",
    "\n",
    "print(f\"Train Qini AUC: {qini_train:.4f}\")\n",
    "print(f\"Test  Qini AUC: {qini_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict uplift on the test set\n",
    "uplift_test = dr_model.predict(X=x_test.values)\n",
    "\n",
    "# Add it to the x_test dataframe for inspection\n",
    "x_test_with_uplift = x_test.copy()\n",
    "x_test_with_uplift[\"uplift\"] = uplift_test\n",
    "\n",
    "# Show top members with highest predicted uplift\n",
    "print(x_test_with_uplift.sort_values(by=\"uplift\", ascending=False).head(10))\n",
    "\n",
    "# Or just inspect some summary statistics\n",
    "print(x_test_with_uplift[\"uplift\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7effbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_with_uplift[\"uplift\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b109dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_with_uplift[\"uplift_clipped\"] = x_test_with_uplift[\"uplift\"].clip(-0.99, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(x_test_with_uplift[\"uplift_clipped\"], bins=50)\n",
    "plt.title(\"Uplift distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "top_members = x_test_with_uplift.nlargest(n, \"uplift_clipped\")\n",
    "print(top_members[[\"uplift_clipped\"]].head(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cf182",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x_test_with_uplift[\"uplift\"] = dr_model.predict(x_test.values)\n",
    "recommended = x_test_with_uplift.nlargest(n, \"uplift\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c90a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge test labels\n",
    "recommended[\"y\"] = y_test.loc[recommended.index]\n",
    "recommended[\"treatment\"] = treatment_test.loc[recommended.index]\n",
    "\n",
    "# Split by treatment in recommended group\n",
    "treated = recommended[recommended[\"treatment\"] == 1]\n",
    "control = recommended[recommended[\"treatment\"] == 0]\n",
    "\n",
    "# Empirical uplift = difference in outcome between treated vs control\n",
    "uplift_empirical = treated[\"y\"].mean() - control[\"y\"].mean()\n",
    "print(\"Empirical uplift for recommended group:\", uplift_empirical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_treated = y_test[treatment_test == 1].mean()\n",
    "overall_control = y_test[treatment_test == 0].mean()\n",
    "overall_uplift = overall_treated - overall_control\n",
    "\n",
    "print(\"Overall uplift in full test set:\", overall_uplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcc1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(recommended[\"uplift\"], bins=30, alpha=0.7, label=\"Recommended\")\n",
    "plt.hist(x_test_with_uplift[\"uplift\"], bins=30, alpha=0.3, label=\"All test\")\n",
    "plt.legend()\n",
    "plt.title(\"Uplift distribution: recommended vs all test\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
